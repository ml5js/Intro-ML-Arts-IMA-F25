# Language Models

- [LLM / Text slides](https://docs.google.com/presentation/d/1uGS4w_UumzQAoq03S-sc9Nn7_hBaRNbQJf1G2SNXooU/edit?usp=sharing)

## Transformers and Large Language Models

> among the reasons I use large pre-trained language models sparingly in my computer-generated poetry practice is that being able to know whose voices I'm speaking with is... actually important, as is being understanding how the output came to have its shape - [@aparrish](https://twitter.com/aparrish/), [full thread](https://twitter.com/aparrish/status/1286808606466244608)

- ğŸ“š [Watch an A.I. Learn to Write by Reading Nothing but **\_\_\_\_**](https://www.nytimes.com/interactive/2023/04/26/upshot/gpt-from-scratch.html) by Aatish Bhatia
- ğŸ“š [Attention is All You Need](https://arxiv.org/abs/1706.03762) - Original "Transformer" paper from 2017, also [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) -- Attention paper from 2014
- ğŸ“š [What Are Transformer Models and How Do They Work?](https://docs.cohere.com/docs/transformer-models)
- ğŸ¥ [How large language models work, a visual intro to transformers](https://youtu.be/wjZofJX0v4M) by 3Blue1Brown
- ğŸ¥ [Intro to Large Language Models](https://youtu.be/zjkBMFhNj_g) by Andrej Karpathy and [Intro to LLMs slides](https://drive.google.com/file/d/1pxx_ZI7O-Nwl7ZLNk5hI3WzAsTLwvNU7/view)
- ğŸ“– [Language Models Can Only Write Ransom Notes](https://posts.decontextualize.com/language-models-ransom-notes/) by Allison Parrish

## LLM Training

- ğŸ¦™ [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
- ğŸ¦™ [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783)
- ğŸ¦œ [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ğŸ¦œ](https://dl.acm.org/doi/10.1145/3442188.3445922)
- ğŸ” [The Foundation Model Transparency Index](https://crfm.stanford.edu/fmti/May-2024/index.html)
- ğŸ“– [Generative AIâ€™s Illusory Case for Fair Use](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4924997) by Jacqueline Charlesworth

### Datasets for LLMs

- ğŸ”¢ [Common Crawl](https://commoncrawl.org/)
- ğŸ”¢ [The Pile](https://pile.eleuther.ai/)

### Climate Impact

- ğŸŒ [The Internetâ€™s Next Great Power Suck](https://www.theatlantic.com/technology/archive/2023/08/ai-carbon-emissions-data-centers/675094/)
- âš¡ï¸ [Carbon Emissions and Large Neural Network Training ](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf)

## Code Examples
- [Word2Vec suggesting alternative words](https://editor.p5js.org/yining/sketches/AqFZmpoAX)
- [Word2vec Similar Words Floating in Space](https://editor.p5js.org/yining/sketches/_Ki2jFoJB)
- [Sentiment](https://editor.p5js.org/ml5/sketches/hopIvsCGL)


## Assignment

- Use one of the LLM APIs to build a chat / agent experience with p5.js
- Or try chat with [Gemini](https://gemini.google.com/app), [Google AI studio](https://aistudio.google.com/prompts/new_chat), [Chatgpt](https://chatgpt.com/), [Claude](https://claude.ai/new) and document your findings. 
- Or add more features to the Word2Vec/Sentiment sketch.
- Document your experminents in a blog post and add a link to the [Assignment Wiki page](https://github.com/ml5js/Intro-ML-Arts-IMA-F25/wiki/Assignment-6). Remember to include visual documentation (screenshots, GIFs, etc)
